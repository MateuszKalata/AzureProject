# AzureProject
 
* Mateusz - LSTM + fasttext https://pypi.org/project/fasttext/
* Natalia - bert https://pypi.org/project/sentence-transformers/
* Jan K - transformer (coś nowego) + glove
* Jan P - roberta https://pypi.org/project/sentence-transformers/
* Jan Ł - elmo https://github.com/allenai/bilm-tf

### Wybrana Bert'a

#### Zadania do zrobienia:

* Douczenie modelu na gotowym zbiorze zdań podobnych 1k lub 10k
* Dobór parametrów modelu 
* Dogenerować zdanai korzystając ze słownika synonimów
* Dobór metody liczenia odległości
* Dobór rozmiaru fragmentów tekstu do przeszukań.
* Wtępne ograniczanie danych
* Ogrnięcie zapisywania stworzonych embedingów

1. Przygotowujemy dema, które będa robić i porównywać embedingi paragrafów z wikipedii. + ograniecie jak doternowywać modele.
Wyświetlą 10 najlepszych wyników wyszukiwania. + badanie wpływu na model rzeczy wymienionych powyżej

#### Zadania krok następny:

* Douczanie pretrenowanych modeli RO/BERTA
* Ocena douczania na podstawie 1000 par zdań
* dobór parametrów douczania/modelu
* badanie skuteczności funkcji ewaluacji (badania podobieństwa)

